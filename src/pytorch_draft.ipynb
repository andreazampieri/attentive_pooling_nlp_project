{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_draft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "r26WdfN-wtwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoSVI7M8_9ay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvCsVwhPOXJ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip WikiQACorpus.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPB7hxUWmLEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as f\n",
        "from torch.autograd import Variable\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-PBzD9QrA0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WikiQA_dataset():\n",
        "  def __init__(self,embedding_dim,batch_size):\n",
        "    self.RAW = data.RawField()\n",
        "    self.TEXT = data.Field(batch_first=True,\n",
        "                           lower=True)\n",
        "    self.LABEL = data.Field(sequential=False,\n",
        "                            unk_token=None)\n",
        "\n",
        "\n",
        "    self.train, self.dev, self.test = data.TabularDataset.splits(\n",
        "        path='.',\n",
        "        train='WikiQA-train.tsv',\n",
        "        validation='WikiQA-dev.tsv',\n",
        "        test='WikiQA-test.tsv',\n",
        "        format='tsv',\n",
        "        fields=[('qid', self.RAW),\n",
        "                ('question', self.TEXT),\n",
        "                ('docid', self.RAW),\n",
        "                ('doctitle', self.RAW),\n",
        "                ('sentenceid',self.RAW),\n",
        "                ('sentence',self.TEXT),\n",
        "                ('label',self.LABEL)])\n",
        "\n",
        "    self.TEXT.build_vocab(self.train, self.dev, self.test, \\\n",
        "                          vectors=GloVe(name='6B', dim=embedding_dim), \\\n",
        "                          unk_init=torch.zeros((1, embedding_dim)).uniform_(-0.25, 0.25))\n",
        "\n",
        "    self.LABEL.build_vocab(self.train)\n",
        "\n",
        "    self.train_iter, self.dev_iter, self.test_iter = \\\n",
        "        data.Iterator.splits((self.train, self.dev, self.test),\n",
        "                             batch_sizes=[batch_size] * 3,\n",
        "                             shuffle=True)\n",
        "\n",
        "\n",
        "class BaselineQACNN(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,hidden_dim,ctx_window,decision_threshold=0.5):\n",
        "    super(BaselineQACNN,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.convolution_q = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window)) # CARE: padding??\n",
        "    self.convolution_a = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window))\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    self.crit = nn.CosineEmbeddingLoss(margin=0.5)\n",
        "    self.decision_threshold = decision_threshold\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.convolution_q(q_embed))\n",
        "    a_enc = F.relu(self.convolution_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    return r_q,r_a\n",
        "  \n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)\n",
        "  \n",
        "  def compute_batch_stats(self,model_output,batch):\n",
        "    r_q,r_a = model_output    \n",
        "    batch_loss = self.crit(r_q,r_a,batch.label.float()*2.0-1) # the loss wants -1|1 values, input values are 0|1\n",
        "    sim = f.cosine_similarity(r_q,r_a)\n",
        "    # compute the decision of the network\n",
        "    pred = sim.clone()\n",
        "    pred[sim > self.decision_threshold] = 1\n",
        "    pred[sim <= self.decision_threshold] = 0\n",
        "    correct_pred = (pred.squeeze() == batch.label.float()).sum().float()\n",
        "    return sim,batch_loss,correct_pred\n",
        "    \n",
        "\n",
        "class BaselineQAbiLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,single_hidden_dim,decision_threshold=0.5):\n",
        "    super(BaselineQAbiLSTM,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.bilstm_q = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.bilstm_a = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    self.crit = nn.CosineEmbeddingLoss(margin=0.5)\n",
        "    self.decision_threshold = decision_threshold\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.bilstm_q(q_embed))\n",
        "    a_enc = F.relu(self.bilstm_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    \n",
        "    return r_q,r_a\n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)\n",
        "  \n",
        "  def compute_batch_stats(self,model_output,batch):\n",
        "    r_q,r_a = model_output    \n",
        "    batch_loss = self.crit(r_q,r_a,batch.label.float()*2.0-1) # the loss wants -1|1 values, input values are 0|1\n",
        "    sim = f.cosine_similarity(r_q,r_a)\n",
        "    # compute the decision of the network\n",
        "    pred = sim.clone()\n",
        "    pred[sim > self.decision_threshold] = 1\n",
        "    pred[sim <= self.decision_threshold] = 0\n",
        "    correct_pred = (pred.squeeze() == batch.label.float()).sum().float()\n",
        "    return sim,batch_loss,correct_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mo_x7Bnl5Sgg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BaselineQACNN(10,400,3,5)\n",
        "wiki_dataset = WikiQA_dataset(100,40)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv6iztrucbqC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "*Draft snippets*\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "nruHT8fI7P62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#a = np.array([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\n",
        "#a = np.array([[[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]],[[[10],[11],[12]],[[13],[14],[15]],[[16],[17],[18]]]])\n",
        "a = np.array([[[[1,2,3],[4,5,6],[7,8,9]]],[[[10,11,12],[13,14,15],[16,17,18]]]])\n",
        "t = torch.from_numpy(a).double()\n",
        "#m = nn.MaxPool2d((1,3))\n",
        "#po = f.max_pool1d(t,t.size(2))\n",
        "#torch.cat(tuple(po),dim=1)\n",
        "#po.view(t.size(0),-1)\n",
        "c = nn.Conv2d(1,1,2).double()\n",
        "c(t)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tL65x3a9W2sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets,transforms\n",
        "data = datasets.MNIST('.',train=True,download=True,transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tW2X_1v7Qg5n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_draft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "r26WdfN-wtwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPB7hxUWmLEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torchtext.vocab import GloVe\n",
        "import torch.nn.functional as f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-PBzD9QrA0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaselineQACNN(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,hidden_dim,ctx_window):\n",
        "    super(BaselineQACNN,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.convolution_q = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window)) # CARE: padding??\n",
        "    self.convolution_a = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window))\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.convolution_q(q_embed))\n",
        "    a_enc = F.relu(self.convolution_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    return f.cosine_similarity(r_q,r_a)\n",
        "  \n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)\n",
        "\n",
        "class BaselineQAbiLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,single_hidden_dim):\n",
        "    super(BaselineQAbiLSTM,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.bilstm_q = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.bilstm_a = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.bilstm_q(q_embed))\n",
        "    a_enc = F.relu(self.bilstm_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    return f.cosine_similarity(r_q,r_a)\n",
        "    \n",
        "    \n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mo_x7Bnl5Sgg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BaselineQACNN(10,400,3,5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv6iztrucbqC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "*Draft snippets*\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "nruHT8fI7P62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#a = np.array([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\n",
        "#a = np.array([[[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]],[[[10],[11],[12]],[[13],[14],[15]],[[16],[17],[18]]]])\n",
        "a = np.array([[[[1,2,3],[4,5,6],[7,8,9]]],[[[10,11,12],[13,14,15],[16,17,18]]]])\n",
        "t = torch.from_numpy(a).double()\n",
        "#m = nn.MaxPool2d((1,3))\n",
        "#po = f.max_pool1d(t,t.size(2))\n",
        "#torch.cat(tuple(po),dim=1)\n",
        "#po.view(t.size(0),-1)\n",
        "c = nn.Conv2d(1,1,2).double()\n",
        "c(t)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tL65x3a9W2sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets,transforms\n",
        "data = datasets.MNIST('.',train=True,download=True,transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoSVI7M8_9ay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_draft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "r26WdfN-wtwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "35c14436-810b-48b5-ccbb-5c2691754d16"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 31kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x598c8000 @  0x7f088826d1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.8MB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, torchtext, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.4.1 torchtext-0.2.3 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EPB7hxUWmLEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torchtext.vocab import GloVe\n",
        "import torch.nn.functional as f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-PBzD9QrA0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaselineQACNN(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,hidden_dim,ctx_window):\n",
        "    super(BaselineQACNN,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.convolution_q = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window)) # CARE: padding??\n",
        "    self.convolution_a = nn.Conv2d(emb_dim,hidden_dim,kernel_size=(emb_dim,ctx_window))\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.convolution_q(q_embed))\n",
        "    a_enc = F.relu(self.convolution_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    return f.cosine_similarity(r_q,r_a)\n",
        "  \n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)\n",
        "\n",
        "class BaselineQAbiLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self,emb_dim,dict_size,single_hidden_dim):\n",
        "    super(BaselineQAbiLSTM,self).__init__()\n",
        "    self.emb = nn.Embedding(dict_size,emb_dim) # with learning, try the GloVe without learning\n",
        "    self.bilstm_q = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.bilstm_a = nn.LSTM(emb_dim,single_hidden_dim,bidirectional=True)\n",
        "    self.h_pool = lambda t : self.horizontal_pooling(t)\n",
        "    \n",
        "  def forward(self,q,a):\n",
        "    q_embed = self.emb(q)\n",
        "    a_embed = self.emb(a)\n",
        "    q_enc = F.relu(self.bilstm_q(q_embed))\n",
        "    a_enc = F.relu(self.bilstm_a(a_embed))\n",
        "    r_q = self.h_pool(q_enc)\n",
        "    r_a = self.h_pool(a_enc)\n",
        "    return f.cosine_similarity(r_q,r_a)\n",
        "    \n",
        "    \n",
        "  # given a matrix, does a maxpool operation on the rows\n",
        "  # t.view(t.size(0),-1) = flatten into a 1D tensor each outcome for each input sample\n",
        "  # passing from (n,a,b) to (n,a) dimensions\n",
        "  def horizontal_pooling(t):\n",
        "    return f.max_pool1d(t,t.size(2)).view(t.size(0),-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mo_x7Bnl5Sgg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BaselineQACNN(10,400,3,5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv6iztrucbqC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "*Draft snippets*\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "nruHT8fI7P62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "02110119-a89d-4e8e-81cb-1dbe38ff4a19"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#a = np.array([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\n",
        "#a = np.array([[[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]],[[[10],[11],[12]],[[13],[14],[15]],[[16],[17],[18]]]])\n",
        "a = np.array([[[[1,2,3],[4,5,6],[7,8,9]]],[[[10,11,12],[13,14,15],[16,17,18]]]])\n",
        "t = torch.from_numpy(a).double()\n",
        "#m = nn.MaxPool2d((1,3))\n",
        "#po = f.max_pool1d(t,t.size(2))\n",
        "#torch.cat(tuple(po),dim=1)\n",
        "#po.view(t.size(0),-1)\n",
        "c = nn.Conv2d(1,1,2).double()\n",
        "c(t)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.4371, -0.1131],\n",
              "          [ 0.5347,  0.8586]]],\n",
              "\n",
              "\n",
              "        [[[ 2.4782,  2.8022],\n",
              "          [ 3.4500,  3.7739]]]],\n",
              "       dtype=torch.float64, grad_fn=<ThnnConv2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "tL65x3a9W2sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets,transforms\n",
        "data = datasets.MNIST('.',train=True,download=True,transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SoSVI7M8_9ay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9d595dd-3e5d-4ff7-d31f-7a84a1d30479"
      },
      "cell_type": "code",
      "source": [
        "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}